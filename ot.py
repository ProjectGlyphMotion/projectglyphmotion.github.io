import cv2
from ultralytics import YOLO
import argparse
from tqdm import tqdm
import torch
import os
import sys
import subprocess
import threading
import time
import queue
import re # Added for ffprobe output parsing

# Attempt to import psutil for CPU/Memory monitoring
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("‚ö†Ô∏è [WARNING] psutil library not found. RAM usage limiting and CPU/Memory utilization display will not be available.")

# Attempt to import GPUtil for GPU monitoring
try:
    import GPUtil
    GPUTIL_AVAILABLE = True
except (ImportError, Exception):
    GPUTIL_AVAILABLE = False


# --- FFMPEG Check ---
FFMPEG_AVAILABLE = False

def check_ffmpeg():
    global FFMPEG_AVAILABLE
    try:
        subprocess.run(["ffmpeg", "-version"], capture_output=True, text=True, check=True)
        print("[INFO] ffmpeg found and seems to be working.")
        FFMPEG_AVAILABLE = True
    except Exception:
        print("‚ö†Ô∏è [WARNING] ffmpeg command not found or not working. Audio processing will be skipped.")
        FFMPEG_AVAILABLE = False
    return FFMPEG_AVAILABLE

def get_video_rotation(video_path: str) -> int:
    """
    Uses ffprobe to get the rotation metadata of a video.
    Returns the rotation angle (0, 90, 180, 270). Returns 0 if no rotation found or on error.
    """
    try:
        command = [
            "ffprobe",
            "-v", "error",
            "-select_streams", "v:0",
            "-show_entries", "stream_tags=rotate",
            "-of", "default=nokey=1:noprint_wrappers=1",
            video_path
        ]
        result = subprocess.run(command, capture_output=True, text=True, check=True, timeout=10)
        rotation_str = result.stdout.strip()
        if rotation_str and rotation_str.isdigit():
            rotation_angle = int(rotation_str)
            if rotation_angle in [0, 90, 180, 270]:
                print(f"[INFO] Detected video rotation: {rotation_angle} degrees.")
                return rotation_angle
        print("[INFO] No rotation metadata found or invalid rotation value, assuming 0 degrees.")
        return 0
    except subprocess.CalledProcessError as e:
        print(f"‚ö†Ô∏è [WARNING] ffprobe failed to get rotation for '{video_path}': {e.stderr.strip()}")
        return 0
    except FileNotFoundError:
        print("‚ö†Ô∏è [WARNING] ffprobe command not found. Cannot determine video rotation.")
        return 0
    except subprocess.TimeoutExpired:
        print(f"‚ö†Ô∏è [WARNING] ffprobe timed out while getting rotation for '{video_path}'.")
        return 0
    except Exception as e:
        print(f"‚ö†Ô∏è [WARNING] An unexpected error occurred while getting video rotation: {e}")
        return 0


# --- Configuration & Setup ---
DEFAULT_MODEL_PATH = "yolov8m.pt"
DEFAULT_OUTPUT_VIDEO_PATH_MARKER = "auto"
DEFAULT_ALLOWED_CLASSES = [
    "person", "car", "truck", "bus", "motorcycle", "bicycle", "airplane", "bird", "cat", "dog",
    "train", "boat", "bench", "backpack", "umbrella", "handbag", "suitcase", "sports ball",
    "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl",
    "chair", "couch", "potted plant", "bed", "dining table",
    "tv", "laptop", "mouse", "remote", "keyboard", "cell phone",
    "microwave", "refrigerator", "book", "clock", "vase", "scissors"
]
DEFAULT_CONFIDENCE_THRESHOLD = 0.25
TEMP_VIDEO_BASENAME = "temp_video_processed_silent.mp4" # This will now be the silent video generated by ffmpeg
OUTPUT_SUBDIRECTORY = "output"
FRAME_QUEUE_SIZE = 30
UTILIZATION_UPDATE_INTERVAL = 1.0 # Interval for updating CPU/Mem/GPU stats in progress bar
# Increased MAX_RAM_USAGE_PERCENT slightly to reduce frequent pausing, use with caution.
MAX_RAM_USAGE_PERCENT = 85.0 # Percentage of total RAM to trigger pause in frame reading

# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
# +++ User Configuration +++
ENABLE_PREVIEW_IN_SCRIPT = False # Set to True to show a live preview window during processing
USE_GPU_IN_SCRIPT = True # Set to True to attempt using GPU (CUDA) for processing if available

# TARGET_PROCESSING_WIDTH: This is the width used for INTERNAL object tracking.
# Set to a lower resolution (e.g., 1920 for 1080p equivalent) to reduce RAM usage and speed up tracking.
# The final output video will be upscaled back to its original resolution.
# Set to None to perform tracking at the original video resolution (higher RAM/slower).
TARGET_PROCESSING_WIDTH = 1920

# FFmpeg Quality Settings (Crucial for High Quality and Reasonable File Size)
# CRF (Constant Rate Factor):
#   0 = truly lossless (pixel-perfect, but results in very large files)
#   16-23 = visually lossless for H.264 (recommended for excellent quality with good compression)
#   Lower CRF means higher quality and larger file size.
FFMPEG_CRF_VALUE = 16 # Set to 0 for bit to bit lossless pixels, or 16-23 for high quality with reasonable file size.
FFMPEG_VIDEO_CODEC = "libx264" # Recommended for wide compatibility. Use "libx265" for HEVC (smaller files, requires x265 encoder).
FFMPEG_PRESET = "medium" # Encoding speed/compression tradeoff: 'ultrafast', 'superfast', 'fast', 'medium', 'slow', 'slower', 'veryslow'

# --- Watermark Configuration ---
# ENABLE_WATERMARK: Master switch to turn watermark on/off
# Set to True to add "Processed by projectglyphmotion.studio" watermark to output videos
# Set to False to disable watermark completely
ENABLE_WATERMARK = True  # ON by default - change to False to disable

# Watermark text - displayed in bottom-right corner
WATERMARK_TEXT = "Processed by projectglyphmotion.studio"

# Watermark appearance settings (dynamic - scales with video resolution)
# Font size is calculated as: video_height / WATERMARK_FONT_DIVISOR
# Higher divisor = smaller text, Lower divisor = larger text
WATERMARK_FONT_DIVISOR = 40  # Results in ~27px for 1080p, ~54px for 4K, ~18px for 720p

# Watermark opacity (0.0 = invisible, 1.0 = fully opaque)
# Recommended: 0.6-0.8 for visible but non-distracting
WATERMARK_OPACITY = 0.7

# Margin from edge (as fraction of video height)
# 0.02 = 2% margin from bottom-right corner
WATERMARK_MARGIN_FRACTION = 0.02
# +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


def get_watermark_filter():
    """
    Generates an FFmpeg drawtext filter for the watermark.
    Uses dynamic expressions so it scales with any video resolution/aspect ratio.
    Portrait videos get smaller text to avoid being too intrusive.
    
    Returns:
        str: FFmpeg filter string for watermark, or empty string if disabled.
    """
    if not ENABLE_WATERMARK:
        return ""
    
    # Escape special characters in the text for FFmpeg
    # FFmpeg drawtext requires escaping colons and backslashes
    escaped_text = WATERMARK_TEXT.replace("\\", "\\\\").replace(":", "\\:")
    
    # Dynamic FFmpeg expressions:
    # h = video height, w = video width, th = text height, tw = text width
    # gt(h,w) = 1 if portrait (height > width), 0 if landscape
    # Font size scales with video dimensions for consistent appearance across resolutions
    # Portrait videos use smaller text (1.5x larger divisor) to be less intrusive
    
    # Calculate divisors for portrait vs landscape
    portrait_divisor = WATERMARK_FONT_DIVISOR * 1.5  # Smaller text for portrait
    landscape_divisor = WATERMARK_FONT_DIVISOR       # Normal text for landscape
    
    # For portrait: use width as reference (since it's smaller dimension)
    # For landscape: use height as reference
    # This ensures watermark is proportional to the smaller dimension
    
    watermark_filter = (
        f"drawtext="
        f"text='{escaped_text}':"
        # Dynamic font size: smaller for portrait, normal for landscape
        # Portrait: min(w,h) / (divisor * 1.5), Landscape: h / divisor
        f"fontsize=if(gt(h\\,w)\\,min(w\\,h)/{portrait_divisor}\\,h/{landscape_divisor}):"
        f"fontcolor=white@{WATERMARK_OPACITY}:"  # White text with configured opacity
        f"x=w-tw-(h*{WATERMARK_MARGIN_FRACTION}):"  # Right-aligned with dynamic margin
        f"y=h-th-(h*{WATERMARK_MARGIN_FRACTION}):"  # Bottom-aligned with dynamic margin
        f"box=1:"  # Enable background box
        f"boxcolor=black@{WATERMARK_OPACITY * 0.5}:"  # Semi-transparent black background
        # Dynamic padding: smaller for portrait
        f"boxborderw=if(gt(h\\,w)\\,min(w\\,h)/{portrait_divisor}/4\\,h/{landscape_divisor}/4)"
    )
    
    print(f"[INFO] Watermark ENABLED: \"{WATERMARK_TEXT}\"")
    print(f"[INFO] Watermark sizing: Landscape={landscape_divisor}, Portrait={portrait_divisor} (smaller)")
    return watermark_filter


def get_system_utilization(device_to_use):
    """
    Retrieves current system utilization statistics (CPU, Memory, GPU).
    Args:
        device_to_use (str): "cuda" or "cpu" to determine if GPU stats should be fetched.
    Returns:
        dict: A dictionary containing utilization percentages and memory usage in GB.
    """
    # Fix: Move global declaration to the top of the function
    global GPUTIL_AVAILABLE

    util_stats = {}
    if PSUTIL_AVAILABLE:
        util_stats['cpu'] = psutil.cpu_percent()
        mem_info = psutil.virtual_memory()
        util_stats['mem_used_gb'] = mem_info.used / (1024**3)
        util_stats['mem_total_gb'] = mem_info.total / (1024**3)
        util_stats['mem'] = mem_info.percent
    if GPUTIL_AVAILABLE and device_to_use == "cuda":
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                util_stats['gpu_load'] = gpu.load * 100
                util_stats['gpu_mem'] = gpu.memoryUtil * 100
        except Exception:
            # GPUtil can sometimes fail if GPU is busy or driver issues.
            # Log and continue without GPU stats.
            print("‚ö†Ô∏è [WARNING] GPUtil failed to get GPU stats.")
            GPUTIL_AVAILABLE = False # Temporarily disable if it fails
    return util_stats

def format_utilization_string(stats):
    """
    Formats system utilization statistics into a human-readable string.
    Args:
        stats (dict): Dictionary of utilization statistics.
    Returns:
        str: Formatted string (e.g., "CPU:50% | Mem:30% (X/YGB) | GPU-L:70% | GPU-M:40%").
    """
    parts = []
    if 'cpu' in stats: parts.append(f"CPU:{stats['cpu']:.1f}%")
    if 'mem' in stats:
        parts.append(f"Mem:{stats['mem']:.1f}% ({stats.get('mem_used_gb',0):.1f}/{stats.get('mem_total_gb',0):.1f}GB)")
    if 'gpu_load' in stats: parts.append(f"GPU-L:{stats['gpu_load']:.1f}%")
    if 'gpu_mem' in stats: parts.append(f"GPU-M:{stats['gpu_mem']:.1f}%")
    return " | ".join(parts) if parts else "Stats N/A"


def frame_reader_thread_func(cap, frame_input_queue, stop_event, original_width, original_height, target_processing_width=None):
    """
    Thread function to read frames from video, resize for processing, and put into a queue.
    Includes a RAM usage check to pause reading if memory is high.
    Args:
        cap (cv2.VideoCapture): OpenCV VideoCapture object.
        frame_input_queue (queue.Queue): Queue to put read/resized frames into.
        stop_event (threading.Event): Event to signal thread to stop.
        original_width (int): Original width of the video as reported by OpenCV.
        original_height (int): Original height of the video as reported by OpenCV.
        target_processing_width (int, optional): Target width for internal processing.
                                                  If None, original resolution is used.
    """
    print("[INFO] Frame reader thread started.")
    count = 0

    # Determine the resolution for internal tracking
    # Note: original_width/height here are the raw dimensions from cap.get(), not necessarily display dimensions.
    processing_width = original_width
    processing_height = original_height
    do_resize_for_processing = False

    if target_processing_width and target_processing_width > 0 and target_processing_width < original_width:
        aspect_ratio = original_height / original_width
        processing_width = target_processing_width
        # Calculate height to maintain aspect ratio, ensuring it's an even number for codecs
        processing_height = int(processing_width * aspect_ratio)
        if processing_height % 2 != 0:
            processing_height = processing_height - 1 if processing_height > 1 else 2 # Ensure even height
        do_resize_for_processing = True
        print(f"[INFO] Frame reader: Resizing frames from {original_width}x{original_height} to {processing_width}x{processing_height} for internal processing.")
    else:
        # If no target_processing_width is set, ensure original dimensions are even for processing compatibility
        if processing_width % 2 != 0: processing_width = processing_width - 1 if processing_width > 1 else 2
        if processing_height % 2 != 0: processing_height = processing_height - 1 if processing_height > 1 else 2
        if original_width != processing_width or original_height != processing_height:
             print(f"‚ö†Ô∏è [WARNING] Frame reader: Adjusted original resolution to {processing_width}x{processing_height} for processing codec compatibility (even dimensions).")
        print(f"[INFO] Frame reader: Processing at original resolution {processing_width}x{processing_height}.")


    while not stop_event.is_set() and cap.isOpened():
        if PSUTIL_AVAILABLE:
            current_ram_usage = psutil.virtual_memory().percent
            ram_check_loops = 0
            # Pause frame reading if RAM usage is too high
            while current_ram_usage > MAX_RAM_USAGE_PERCENT and not stop_event.is_set():
                if ram_check_loops % 5 == 0: # Print warning every 5 seconds of pause
                    print(f"‚ö†Ô∏è [WARNING] [FRAME_READER] High RAM usage: {current_ram_usage:.1f}%. Pausing frame reading for 1s...")
                time.sleep(1.0)
                current_ram_usage = psutil.virtual_memory().percent
                ram_check_loops +=1
                if stop_event.is_set():
                    print("[INFO] [FRAME_READER] Stop event received during RAM pause.")
                    # Fix: Ensure stop signal is consistent (2 values)
                    if not frame_input_queue.full(): frame_input_queue.put((False, None))
                    return

        # Only read frame if there's space in the queue
        if frame_input_queue.qsize() < FRAME_QUEUE_SIZE:
            ret, frame = cap.read()
            if not ret: # End of video or error reading frame
                print(f"[INFO] Frame reader: End of video or cannot read frame after {count} frames.")
                # Fix: Ensure end signal is consistent (2 values)
                frame_input_queue.put((False, None))
                break

            # Resize frame for internal processing if target_processing_width is set
            if do_resize_for_processing:
                try:
                    processed_frame = cv2.resize(frame, (processing_width, processing_height), interpolation=cv2.INTER_AREA)
                except Exception as e:
                    print(f"‚ùå [ERROR] [FRAME_READER] Failed to resize frame for processing: {e}. Using original frame.")
                    processed_frame = frame # Fallback to original if resize fails
            else:
                processed_frame = frame

            # Put the processed_frame (at tracking resolution) into the queue
            frame_input_queue.put((True, processed_frame))
            count += 1
        else:
            time.sleep(0.005) # Small delay to prevent busy-waiting if queue is full

    # Ensure stop signal is sent if loop finishes naturally
    if not stop_event.is_set() and not frame_input_queue.full():
        frame_input_queue.put((False, None))
    print(f"[INFO] Frame reader thread finished after reading {count} frames.")


def frame_writer_thread_func(ffmpeg_stdin_pipe, frame_output_queue, stop_event):
    """
    Writes processed and (upscaled) frames from queue directly to FFmpeg's stdin pipe.
    Args:
        ffmpeg_stdin_pipe (subprocess.PIPE): stdin pipe of the FFmpeg process.
        frame_output_queue (queue.Queue): Queue to get (upscaled) frames from.
        stop_event (threading.Event): Event to signal thread to stop.
    """
    print("[INFO] Frame writer thread started for FFmpeg pipe.")
    count = 0
    try:
        while not stop_event.is_set():
            try:
                # Get frame from queue with a timeout to allow checking stop_event
                ret, frame_to_write = frame_output_queue.get(timeout=0.1)
                if not ret: # End signal received
                    print(f"[INFO] Frame writer: End signal received after writing {count} frames.")
                    break # Exit loop

                if frame_to_write is not None:
                    # Convert frame to raw bytes for FFmpeg (BGR is OpenCV default)
                    ffmpeg_stdin_pipe.write(frame_to_write.tobytes())
                    count += 1
                frame_output_queue.task_done()
            except queue.Empty:
                # If queue is empty, check stop_event and if the main loop finished
                if stop_event.is_set() and frame_output_queue.empty():
                    print("[INFO] Frame writer: Stop event set and queue empty.")
                    break
                continue # Keep waiting for frames
            except BrokenPipeError:
                print("‚ùå [ERROR] [FRAME_WRITER] Broken pipe to FFmpeg. FFmpeg likely exited unexpectedly.")
                break
            except Exception as e:
                print(f"‚ùå [ERROR] [FRAME_WRITER] Error writing frame to FFmpeg pipe: {e}")
                break
    finally:
        # Crucially, close the pipe to signal FFmpeg that no more frames are coming
        if ffmpeg_stdin_pipe:
            try:
                ffmpeg_stdin_pipe.close()
                print("[INFO] FFmpeg stdin pipe closed.")
            except Exception as e:
                print(f"‚ö†Ô∏è [WARNING] Error closing FFmpeg stdin pipe: {e}")

        # Mark remaining tasks as done if any are left (e.g. if an error occurred)
        while not frame_output_queue.empty():
            try:
                frame_output_queue.get_nowait()
                frame_output_queue.task_done()
            except queue.Empty:
                break # Queue is now truly empty

    print(f"[INFO] Frame writer thread finished. Total frames written to FFmpeg: {count}")


def parse_arguments():
    """Parses command-line arguments for the script."""
    parser = argparse.ArgumentParser(description="Object Tracking: YOLOv8, Threaded I/O, GPU/CPU, Progress, ffmpeg Audio, Utilization")
    parser.add_argument("--model",type=str,default=DEFAULT_MODEL_PATH,help="Path to YOLOv8 model.")
    parser.add_argument("--output_video",type=str,default=DEFAULT_OUTPUT_VIDEO_PATH_MARKER,help="Path for final video. Default: 'auto' (derived from input name, saved in 'output/' subdir).")
    parser.add_argument("--allowed_classes",nargs="+",default=DEFAULT_ALLOWED_CLASSES,help=f"Classes to track.")
    parser.add_argument("--confidence_threshold",type=float,default=DEFAULT_CONFIDENCE_THRESHOLD,help=f"Min confidence.")
    parser.add_argument("--input_video", type=str, required=True, help="Path to the input video file.")
    return parser.parse_args()

def start_ffmpeg_video_encoder(output_path, width, height, fps, vf_filter_string=""):
    """
    Starts an FFmpeg subprocess to encode raw video frames from stdin, applying filters if needed.
    Args:
        output_path (str): Full path for the output video file.
        width (int): Width of the video frames FFmpeg will receive from stdin (raw encoded width).
        height (int): Height of the video frames FFmpeg will receive from stdin (raw encoded height).
        fps (float): Frame rate of the video.
        vf_filter_string (str): FFmpeg video filter string (e.g., "transpose=1").
    Returns:
        subprocess.Popen: FFmpeg subprocess object.
    """
    ffmpeg_cmd = [
        "ffmpeg",
        "-y", # Overwrite output file without asking
        "-hide_banner", # Hide FFmpeg startup banner
        "-loglevel", "error", # Only show errors
        "-stats", # Show progress stats (prints to stderr)

        # Input Options (raw video from stdin)
        "-f", "rawvideo",
        "-pix_fmt", "bgr24", # OpenCV's default format (BGR 8-bit per channel)
        "-s", f"{width}x{height}", # Resolution FFmpeg expects from stdin (RAW dimensions from OpenCV)
        "-r", str(fps), # Frame rate
        "-i", "-", # Input from stdin
    ]

    # Add video filter if provided
    if vf_filter_string:
        ffmpeg_cmd.extend(["-vf", vf_filter_string])

    # Output Options (encoded video)
    ffmpeg_cmd.extend([
        "-c:v", FFMPEG_VIDEO_CODEC, # Video codec (e.g., libx264, libx265)
        "-preset", FFMPEG_PRESET, # Encoding speed/compression tradeoff
        "-crf", str(FFMPEG_CRF_VALUE), # Constant Rate Factor (quality setting)
        "-pix_fmt", "yuv420p", # Pixel format for wider compatibility (e.g., QuickTime)
        output_path
    ])

    print(f"[INFO] Starting FFmpeg video encoder. Command: {' '.join(ffmpeg_cmd)}")
    # stderr=subprocess.PIPE is crucial to capture FFmpeg's progress and error messages
    process = subprocess.Popen(ffmpeg_cmd, stdin=subprocess.PIPE, stderr=subprocess.PIPE)
    return process


def process_audio_ffmpeg(video_source_path, temp_silent_video_path, final_output_video_path):
    """
    Copies audio from original video and merges it with the processed silent video.
    Args:
        video_source_path (str): Path to the original input video (for audio).
        temp_silent_video_path (str): Path to the processed video (without audio, but now correctly oriented).
        final_output_video_path (str): Desired path for the final video with audio.
    Returns:
        bool: True if audio merge was successful, False otherwise.
    """
    if not FFMPEG_AVAILABLE:
        print("‚ö†Ô∏è [WARNING] [AUDIO_MERGE] ffmpeg not available. Skipping audio merge.")
        # If no ffmpeg, and silent video is the direct output from tracking, rename it.
        if os.path.exists(temp_silent_video_path) and not os.path.exists(final_output_video_path):
            try:
                os.rename(temp_silent_video_path, final_output_video_path)
                print(f"‚úÖ [SUCCESS] Processed silent video saved as final output: '{final_output_video_path}'.")
            except OSError as e:
                print(f"‚ùå [ERROR] Could not rename temp silent file: {e}.")
        return False

    if not os.path.exists(temp_silent_video_path):
        print(f"‚ùå [ERROR] [AUDIO_MERGE] Temporary silent video '{temp_silent_video_path}' not found. Cannot merge audio.")
        return False

    print(f"\n[AUDIO_MERGE] Attempting to extract and merge audio using ffmpeg.")
    if os.path.exists(final_output_video_path):
        print(f"‚ö†Ô∏è [WARNING] Output file {final_output_video_path} exists. Overwriting for audio merge.")

    # Use a temporary name for the audio-merged file
    final_output_video_temp = final_output_video_path + ".temp_audio_merged.mp4"

    # Command to copy video stream and audio stream (if present)
    # -map 0:v:0 ensures only the video stream from the first input (temp_silent_video_path) is used
    # -map 1:a:0? tries to use the first audio stream from the second input (video_source_path), '?' makes it optional
    # -shortest ensures output duration is limited by the shortest input stream
    # -c:v copy copies the video stream without re-encoding (since it was just encoded by FFmpeg pipe)
    # -c:a aac re-encodes audio to AAC for wide compatibility
    # No -vf filter needed here as the temp_silent_video_path is already correctly oriented.
    ffmpeg_audio_merge_cmd = [
        "ffmpeg", "-y", "-i", temp_silent_video_path, "-i", video_source_path,
        "-map", "0:v:0", "-map", "1:a:0?", # Map video from first input, audio from second
        "-c:v", "copy",
        "-c:a", "aac",
        "-strict", "experimental", # Needed for some AAC encoders
        "-b:a", "192k", # Audio bitrate, adjust as needed
        "-shortest",
        final_output_video_temp
    ]

    try:
        print(f"[AUDIO_MERGE] Executing audio merge: {' '.join(ffmpeg_audio_merge_cmd)}")
        # Capture output but don't print unless there's an error
        result = subprocess.run(ffmpeg_audio_merge_cmd, check=True, capture_output=True, text=True)
        # If check=True, CalledProcessError is raised for non-zero exit codes.
        # So, if we reach here, it was successful, no need to print stdout/stderr.

        if os.path.exists(final_output_video_temp):
            # If the temporary file was created, move it to the final destination
            if os.path.exists(final_output_video_path):
                os.remove(final_output_video_path)
            os.rename(final_output_video_temp, final_output_video_path)
            print(f"‚úÖ [SUCCESS] Audio merged and final video saved: '{final_output_video_path}'")
            return True
        else:
            print(f"‚ùå [ERROR] [AUDIO_MERGE] FFmpeg did not create expected output file '{final_output_video_temp}'.")
            return False

    except subprocess.CalledProcessError as e:
        print(f"‚ùå [ERROR] [AUDIO_MERGE] FFmpeg audio merge failed with exit code {e.returncode}.")
        print(f"STDOUT:\n{e.stdout}")
        print(f"STDERR:\n{e.stderr}")
    except Exception as e_ffmpeg:
        print(f"‚ùå [ERROR] [AUDIO_MERGE] An unexpected error occurred during FFmpeg audio merge: {e_ffmpeg}")

    return False

def main():
    start_time_total = time.time()

    # Define the absolute path for the temporary silent video output
    temp_silent_video_abs_path = os.path.abspath(os.path.join(OUTPUT_SUBDIRECTORY, TEMP_VIDEO_BASENAME))

    # Initialize all critical variables at the top to prevent UnboundLocalError
    processing_loop_active = True
    frame_count = 0
    last_printed_percentage = -1 # Not used with tqdm
    original_width = 0
    original_height = 0
    fps = 0.0
    total_frames = 0
    model = None # Initialize model to None, assigned later in try/except


    # --- PyTorch CPU Threading Configuration ---
    # Attempts to optimize PyTorch's CPU operations by setting the number of threads.
    try:
        cpu_cores = os.cpu_count()
        if cpu_cores:
            num_threads_to_set = max(1, cpu_cores // 2) # Use half of logical cores for balance
            torch.set_num_threads(num_threads_to_set)
            print(f"[INFO] Suggested {num_threads_to_set} threads for PyTorch CPU operations.")
        else:
            print("[INFO] Could not determine CPU core count for PyTorch thread setting.")
    except Exception as e:
        print(f"‚ö†Ô∏è [WARNING] Could not set PyTorch CPU threads: {e}")

    # Initial checks for FFmpeg and monitoring libraries
    print("--- FFMPEG Check ---"); check_ffmpeg(); print("--------------------")
    if PSUTIL_AVAILABLE: print("[DEBUG] Priming psutil.cpu_percent()..."); psutil.cpu_percent()
    if GPUTIL_AVAILABLE:
        print("[DEBUG] Attempting to prime GPUtil.getGPUs()...")
        try: GPUtil.getGPUs(); print("[DEBUG] GPUtil.getGPUs() primed.")
        except Exception as e: print(f"[DEBUG] Error GPUtil priming: {e}")

    args = parse_arguments()

    current_input_video = args.input_video

    # Adjust input video path if it's not absolute and not found in current dir (for Telegram bot)
    if not os.path.exists(current_input_video) and not os.path.isabs(current_input_video):
        potential_input_path = os.path.join("input", current_input_video)
        if os.path.exists(potential_input_path) and os.path.isfile(potential_input_path):
            current_input_video = potential_input_path
            print(f"[INFO] Input path adjusted to: {current_input_video}")

    # Validate input video file
    if not current_input_video:
        print("‚ùå [ERROR] No input video path provided. Exiting."); return
    if not os.path.exists(current_input_video) or not os.path.isfile(current_input_video):
        print(f"‚ùå [ERROR] Input video file not found or is not a file: {current_input_video}"); return

    show_preview = ENABLE_PREVIEW_IN_SCRIPT
    use_gpu = USE_GPU_IN_SCRIPT

    # Create output directory if it doesn't exist
    os.makedirs(OUTPUT_SUBDIRECTORY, exist_ok=True)

    # Determine the final output video path and name
    final_output_video_path = args.output_video
    if args.output_video == DEFAULT_OUTPUT_VIDEO_PATH_MARKER:
        input_basename_only = os.path.basename(current_input_video)
        name, ext = os.path.splitext(input_basename_only)
        # Always output as .mp4 for consistency and compatibility
        base_name = f"{name}_processed.mp4"
        final_output_video_path = os.path.join(OUTPUT_SUBDIRECTORY, base_name)
    else:
        if not os.path.isabs(args.output_video):
            final_output_video_path = os.path.join(OUTPUT_SUBDIRECTORY, os.path.basename(args.output_video))
        # Ensure output is .mp4 if a custom extension was provided (otherwise FFmpeg might default to .mkv)
        if not final_output_video_path.lower().endswith('.mp4'):
            final_output_video_path = os.path.splitext(final_output_video_path)[0] + '.mp4'

    print(f"[INFO] Final output video will be saved as: {final_output_video_path}")

    # Determine which device (CPU/GPU) to use for YOLO model
    device_to_use = "cpu"
    if use_gpu and torch.cuda.is_available():
        device_to_use = "cuda"
        print("‚úÖ [SUCCESS] CUDA GPU available. Using GPU.")
    elif use_gpu:
        print("‚ö†Ô∏è [WARNING] CUDA GPU not found. Falling back to CPU.")
    else:
        print("‚ÑπÔ∏è [INFO] Using CPU.")

    # Load YOLO model
    print(f"[INFO] Loading model: {args.model} on {device_to_use}")
    try:
        model = YOLO(args.model)
        model.to(device_to_use)
        print(f"‚úÖ [SUCCESS] Model loaded.")
    except Exception as e:
        print(f"‚ùå [ERROR] Failed to load model: {e}"); return

    print(f"[INFO] Tracking classes: {args.allowed_classes}")

    # Open video capture
    cap = cv2.VideoCapture(current_input_video)
    if not cap.isOpened():
        print(f"‚ùå [ERROR] Failed to open input video file: {current_input_video}"); return

    # Get original video properties and assign to initialized variables
    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    if fps == 0.0 or fps is None: # Fallback for unreadable FPS
        fps = 30.0; print(f"‚ö†Ô∏è [WARNING] FPS not readable, defaulting to {fps} FPS.")
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Assign total_frames here

    # --- Determine Rotation and Final Output Dimensions ---
    rotation_angle = get_video_rotation(current_input_video)
    vf_filter = "" # FFmpeg video filter for rotation correction

    # These are the dimensions that FFmpeg will output *after* applying the rotation filter.
    # So, if original is 1920x1080 (landscape) but needs to be 1080x1920 (portrait),
    # ffmpeg_output_width should be 1080 and ffmpeg_output_height should be 1920.
    # OpenCV's cap.get() returns the raw encoded dimensions.
    ffmpeg_output_width = original_width
    ffmpeg_output_height = original_height

    if rotation_angle == 90:
        vf_filter = "transpose=1" # Rotate 90 degrees clockwise (for portrait videos encoded landscape with rotate 90)
        ffmpeg_output_width = original_height # Swap for display dimensions
        ffmpeg_output_height = original_width
        print(f"[INFO] Applying FFmpeg rotation filter: {vf_filter}. Output dimensions will be {ffmpeg_output_width}x{ffmpeg_output_height}.")
    elif rotation_angle == 270:
        vf_filter = "transpose=2" # Rotate 90 degrees counter-clockwise
        ffmpeg_output_width = original_height # Swap for display dimensions
        ffmpeg_output_height = original_width
        print(f"[INFO] Applying FFmpeg rotation filter: {vf_filter}. Output dimensions will be {ffmpeg_output_width}x{ffmpeg_output_height}.")
    elif rotation_angle == 180:
        vf_filter = "transpose=2,transpose=2" # Rotate 180 degrees
        # Dimensions remain the same as 180 degree rotation doesn't swap width/height
        print(f"[INFO] Applying FFmpeg rotation filter: {vf_filter}. Output dimensions will remain {ffmpeg_output_width}x{ffmpeg_output_height}.")

    # --- Add Watermark Filter ---
    # Watermark is applied AFTER rotation so it appears correctly positioned in final video
    watermark_filter = get_watermark_filter()
    if watermark_filter:
        if vf_filter:
            # Chain filters: rotation first, then watermark
            vf_filter = f"{vf_filter},{watermark_filter}"
        else:
            vf_filter = watermark_filter
        print(f"[INFO] Final FFmpeg filter chain: {vf_filter[:80]}..." if len(vf_filter) > 80 else f"[INFO] Final FFmpeg filter chain: {vf_filter}")
    else:
        print("[INFO] Watermark DISABLED.")

    # Ensure FFmpeg output dimensions are even for codec compatibility.
    if ffmpeg_output_width % 2 != 0:
        ffmpeg_output_width -= 1
        print(f"‚ö†Ô∏è [WARNING] Adjusted final output width to {ffmpeg_output_width} for FFmpeg codec compatibility (even dimension).")
    if ffmpeg_output_height % 2 != 0:
        ffmpeg_output_height -= 1
        print(f"‚ö†Ô∏è [WARNING] Adjusted final output height to {ffmpeg_output_height} for FFmpeg codec compatibility (even dimension).")

    # --- Internal Tracking Resolution (for YOLO model processing) ---
    # This is what the frame_reader_thread_func will resize frames to.
    # These internal dimensions are derived from original_width/height as reported by OpenCV,
    # because the actual frames read by OpenCV are still in their raw encoded orientation.
    effective_tracking_width = original_width
    effective_tracking_height = original_height
    target_width_for_reader = None

    if TARGET_PROCESSING_WIDTH and TARGET_PROCESSING_WIDTH > 0 and TARGET_PROCESSING_WIDTH < original_width:
        aspect_ratio = original_height / original_width
        target_width_for_reader = TARGET_PROCESSING_WIDTH # Reader will resize to this width
        effective_tracking_width = TARGET_PROCESSING_WIDTH
        effective_tracking_height = int(effective_tracking_width * aspect_ratio)
        if effective_tracking_height % 2 != 0:
            effective_tracking_height = effective_tracking_height - 1 if effective_tracking_height > 1 else 2
        print(f"[INFO] Internal object tracking will be performed on frames resized to {effective_tracking_width}x{effective_tracking_height}.")
    else:
        if effective_tracking_width % 2 != 0: effective_tracking_width = effective_tracking_width - 1 if effective_tracking_width > 1 else 2
        if effective_tracking_height % 2 != 0: effective_tracking_height = effective_tracking_height - 1 if effective_tracking_height > 1 else 2
        print(f"[INFO] Internal object tracking will be performed on frames at raw original resolution: {effective_tracking_width}x{effective_tracking_height}.")


    print(f"[INFO] FFmpeg final output video will be {ffmpeg_output_width}x{ffmpeg_output_height} at {fps:.2f} FPS (after rotation correction).")
    print(f"[INFO] FFmpeg encoding with codec: {FFMPEG_VIDEO_CODEC}, CRF: {FFMPEG_CRF_VALUE}, Preset: {FFMPEG_PRESET}.")

    # Start FFmpeg subprocess for encoding processed frames at the FINAL output resolution
    # Pass the determined filter string and the target final output dimensions
    ffmpeg_process = start_ffmpeg_video_encoder(temp_silent_video_abs_path, original_width, original_height, fps, vf_filter_string=vf_filter)
    if not ffmpeg_process:
        print("‚ùå [ERROR] Failed to start FFmpeg video encoder. Exiting."); cap.release(); return
    print(f"‚úÖ [SUCCESS] FFmpeg encoder started (PID: {ffmpeg_process.pid}).")


    # Setup threading queues and events
    stop_event = threading.Event()
    frame_input_queue = queue.Queue(maxsize=FRAME_QUEUE_SIZE) # Queue for frames read from video (at tracking resolution)
    frame_output_queue = queue.Queue(maxsize=FRAME_QUEUE_SIZE) # Queue for processed frames (upscaled to final output resolution)

    # Start frame reader thread (reads and resizes to tracking resolution)
    reader_thread = threading.Thread(
        target=frame_reader_thread_func,
        args=(cap, frame_input_queue, stop_event, original_width, original_height, target_width_for_reader)
    )
    reader_thread.daemon = True # Allows program to exit even if this thread is still running
    reader_thread.start()

    # Start frame writer thread (writes frames to FFmpeg pipe)
    writer_thread = threading.Thread(
        target=frame_writer_thread_func,
        args=(ffmpeg_process.stdin, frame_output_queue, stop_event)
    )
    writer_thread.daemon = True
    writer_thread.start()

    # Setup tqdm progress bar for main processing loop
    pbar_desc = f"Processing Frames ({device_to_use.upper()})"
    pbar = tqdm(total=total_frames, unit="frame",
                desc=pbar_desc,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]",
                mininterval=0) # mininterval=0 for instant updates. No '\n' in bar_format for single line.

    print(f"üöÄ Starting video frame processing on {device_to_use.upper()}...")

    try:
        while processing_loop_active:
            try:
                # Get frame from input queue (this frame is at effective_tracking_width/height)
                ret, frame_to_process = frame_input_queue.get(timeout=1.0)
                if not ret: # End signal or error from reader thread
                    processing_loop_active = False
                    break
                frame_input_queue.task_done()
            except queue.Empty:
                # If queue is empty and reader thread is no longer active, then processing is done
                if not reader_thread.is_alive() and frame_input_queue.empty():
                    processing_loop_active = False;
                    break
                continue # Keep waiting for frames

            frame_count += 1

            # Update utilization stats and progress bar
            util_stats = get_system_utilization(device_to_use)
            pbar.set_postfix_str(format_utilization_string(util_stats), refresh=True)
            pbar.update(1) # Update the tqdm progress bar for each frame processed


            # Perform object tracking on the (potentially lower resolution) frame
            results = model.track(frame_to_process, persist=True, verbose=False, conf=args.confidence_threshold)

            # Create a copy of the frame to draw annotations on
            annotated_frame = frame_to_process.copy()

            # Draw bounding boxes and labels if objects are detected
            if results and results[0].boxes:
                for box in results[0].boxes:
                    if box.id is None: continue # Skip if no tracking ID
                    cls_id = int(box.cls[0])
                    class_name = model.names[cls_id]
                    track_id = int(box.id[0])

                    # Only draw for allowed classes
                    if class_name in args.allowed_classes:
                        x1,y1,x2,y2=map(int,box.xyxy[0]) # Bounding box coordinates
                        conf=box.conf[0] # Confidence score
                        label=f"ID:{track_id} {class_name} {conf:.2f}"

                        cv2.rectangle(annotated_frame,(x1,y1),(x2,y2),(0,255,0),2) # Green rectangle
                        (tw,th),_=cv2.getTextSize(label,cv2.FONT_HERSHEY_SIMPLEX,0.5,1)
                        cv2.rectangle(annotated_frame,(x1,y1-th-10),(x1+tw,y1-5),(0,255,0),-1) # Background for text
                        cv2.putText(annotated_frame,label,(x1,y1-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,0),1) # Black text

            # --- Upscale the annotated frame to the final output resolution ---
            annotated_frame_for_output = annotated_frame
            # Only resize if the current frame's dimensions don't match the desired FFmpeg input dimensions.
            # FFmpeg is told to expect `original_width` x `original_height` raw frames.
            if annotated_frame.shape[1] != original_width or annotated_frame.shape[0] != original_height:
                annotated_frame_for_output = cv2.resize(
                    annotated_frame,
                    (original_width, original_height), # Resize to raw dimensions for FFmpeg input
                    interpolation=cv2.INTER_LINEAR
                )
                # print(f"[DEBUG] Resized frame from {annotated_frame.shape[1]}x{annotated_frame.shape[0]} to {original_width}x{original_height} for FFmpeg input.")

            # Put the upscaled annotated frame into the output queue for the writer thread
            frame_output_queue.put((True, annotated_frame_for_output))

            # Show preview window if enabled
            if show_preview:
                # The preview window should show the final, correctly oriented output.
                # So we need to apply the rotation filter here *for display only*.
                preview_frame_display = annotated_frame_for_output.copy()
                if rotation_angle == 90:
                    preview_frame_display = cv2.rotate(preview_frame_display, cv2.ROTATE_90_CLOCKWISE)
                elif rotation_angle == 270:
                    preview_frame_display = cv2.rotate(preview_frame_display, cv2.ROTATE_90_COUNTERCLOCKWISE)
                elif rotation_angle == 180:
                    preview_frame_display = cv2.rotate(preview_frame_display, cv2.ROTATE_180)
                
                cv2.imshow("Object Tracking", preview_frame_display)
                if cv2.waitKey(1)&0xFF==ord("q"):
                    processing_loop_active=False;
                    break
            del results # Free up memory from YOLO results

    except KeyboardInterrupt:
        print("\n[INFO] KeyboardInterrupt. Stopping processing...");
        processing_loop_active = False
    except Exception as e:
        # Crucially, set processing_loop_active to False IMMEDIATELY when an error occurs in this outer except block.
        processing_loop_active = False # Ensure this is the first thing in the except block
        print(f"‚ùå [ERROR] An unexpected error occurred in main processing loop: {e}")
    finally:
        print("\n[INFO] Main loop finished. Signaling I/O threads to stop...") # Added newline for cleaner output
        if stop_event: stop_event.set() # Signal reader/writer threads to stop

        # Ensure reader thread finishes
        if reader_thread is not None and reader_thread.is_alive():
            print("[INFO] Waiting for frame reader thread...");
            reader_thread.join(timeout=5.0) # Give it some time to finish
            if reader_thread.is_alive(): print("‚ö†Ô∏è [WARNING] Frame reader didn't finish in time.")

        # Explicitly put a stop signal for the writer, then join it.
        # The writer will close the pipe when it gets this signal or finishes.
        if frame_output_queue is not None:
             frame_output_queue.put((False, None)) # Ensure writer gets the stop signal

        # Ensure writer thread finishes
        if writer_thread is not None and writer_thread.is_alive():
            print("[INFO] Waiting for frame writer thread...");
            writer_thread.join(timeout=10.0) # Give it more time as it might be writing to disk
            if writer_thread.is_alive(): print("‚ö†Ô∏è [WARNING] Frame writer didn't finish in time.")

        pbar.close() # Close tqdm progress bar properly
        if cap.isOpened(): cap.release()
        if show_preview: cv2.destroyAllWindows()

        # Wait for FFmpeg process to finish and check its return code
        # The writer thread should have closed stdin, but ensure it here too for robustness
        try:
            if ffmpeg_process.stdin:
                ffmpeg_process.stdin.close()
        except Exception as e:
            print(f"‚ö†Ô∏è [WARNING] Error closing ffmpeg_process stdin in finally block: {e}")

        ffmpeg_stderr_output = ffmpeg_process.stderr.read().decode('utf-8', errors='ignore')
        ffmpeg_return_code = ffmpeg_process.wait() # Wait for FFmpeg to complete encoding

        if ffmpeg_return_code == 0:
            print(f"‚úÖ [SUCCESS] FFmpeg video encoding completed to '{temp_silent_video_abs_path}'.")
        else:
            print(f"‚ùå [ERROR] FFmpeg video encoding failed with exit code {ffmpeg_return_code}.")
            print(f"FFmpeg stderr:\n{ffmpeg_stderr_output}")
            # If encoding failed, the temp_silent_video_abs_path might be incomplete or invalid
            if os.path.exists(temp_silent_video_abs_path):
                try:
                    os.remove(temp_silent_video_abs_path)
                    print(f"[CLEANUP] Removed incomplete encoded video '{temp_silent_video_abs_path}'.")
                except OSError as e:
                    print(f"‚ö†Ô∏è [WARNING] Error deleting incomplete video '{temp_silent_video_abs_path}': {e}")
            return # Exit main if video encoding failed

    print(f"‚úÖ [SUCCESS] Video frame processing complete. Silent video encoded to: '{temp_silent_video_abs_path}'.")
    print(f"[INFO] Total frames processed: {frame_count}")

    audio_processing_was_successful = False
    if FFMPEG_AVAILABLE:
        print("[INFO] Starting audio processing...")
        # Merge audio from the original video with the processed silent video
        # The temp_silent_video_abs_path is already correctly oriented, so no need for audio_processing_ffmpeg to apply filters.
        audio_processing_was_successful = process_audio_ffmpeg(current_input_video, temp_silent_video_abs_path, final_output_video_path)
    else:
        # If FFmpeg is not available, the temporary silent video is the final output
        if os.path.exists(temp_silent_video_abs_path):
            try:
                # Ensure the target final output path exists before renaming (remove if it does)
                if os.path.exists(final_output_video_path):
                    os.remove(final_output_video_path)
                os.rename(temp_silent_video_abs_path, final_output_video_path)
                print(f"‚úÖ [SUCCESS] Silent video saved as '{final_output_video_path}'.")
                audio_processing_was_successful = True
            except OSError as e:
                print(f"‚ùå [ERROR] Could not rename temp silent file to final path: {e}.")

    # Final cleanup of the temporary silent video file
    if os.path.exists(temp_silent_video_abs_path):
        # Only remove if audio processing was successful (meaning the temp file was moved/copied)
        # or if the temp file path is different from the final output path (to avoid deleting the final result)
        if audio_processing_was_successful and os.path.abspath(temp_silent_video_abs_path) != os.path.abspath(final_output_video_path):
            try:
                os.remove(temp_silent_video_abs_path)
                print(f"[CLEANUP] Temp silent video '{temp_silent_video_abs_path}' deleted.")
            except OSError as e:
                print(f"‚ö†Ô∏è [WARNING] Error deleting temporary file '{temp_silent_video_abs_path}': {e}")
        elif not audio_processing_was_successful and os.path.abspath(temp_silent_video_abs_path) == os.path.abspath(final_output_video_path):
            print(f"[INFO] Audio processing failed/skipped. Output is the processed silent video: '{final_output_video_path}'.")
        elif not audio_processing_was_successful:
             print(f"[INFO] Audio processing failed/skipped. Temporary file '{temp_silent_video_abs_path}' is kept for inspection.")

    end_time_total = time.time()
    total_processing_time = end_time_total - start_time_total
    minutes = int(total_processing_time // 60)
    seconds = int(total_processing_time % 60)
    print(f"\n[DONE] Total script execution time: {minutes} minute(s) and {seconds} second(s).")
    print(f"[INFO] Final output video is at: {final_output_video_path}")

if __name__ == "__main__":
    main()
